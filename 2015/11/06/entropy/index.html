<!DOCTYPE html>
<html>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. H">
<meta name="keywords" content="entropy">
<meta property="og:type" content="article">
<meta property="og:title" content="Feature Selection using Mutual Information">
<meta property="og:url" content="http://luofanghao.github.io/blog/2015/11/06/entropy/index.html">
<meta property="og:site_name" content="BLOG">
<meta property="og:description" content="Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. H">
<meta property="og:updated_time" content="2016-07-01T08:44:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Feature Selection using Mutual Information">
<meta name="twitter:description" content="Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. H">
    
    
        
          
              <link rel="shortcut icon" href="/blog/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/blog/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Feature Selection using Mutual Information</title>
    <!-- styles -->
    <link rel="stylesheet" href="/blog/css/style.css">
    <!-- rss -->
    
    
</head>

<body>
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="http://luofanghao.github.io">Home</a></li>
         
          <li><a href="/blog/">Blog</a></li>
         
          <li><a href="/blog/archives/">Writing</a></li>
         
          <li><a href="/blog/about/">AboutMe</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/blog/2016/06/27/caffe使用总结/"><i class="fa fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fa fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://luofanghao.github.io/blog/2015/11/06/entropy/"><i class="fa fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&text=Feature Selection using Mutual Information"><i class="fa fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&is_video=false&description=Feature Selection using Mutual Information"><i class="fa fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Feature Selection using Mutual Information&body=Check out this article: http://luofanghao.github.io/blog/2015/11/06/entropy/"><i class="fa fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&name=Feature Selection using Mutual Information&description=&lt;p&gt;Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. Here we will introduce the basic concept of this.&lt;/p&gt;"><i class="fa fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Theory"><span class="toc-number">1.</span> <span class="toc-text">Theory</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Shannon-Measure-of-Information-SIC"><span class="toc-number">1.1.</span> <span class="toc-text">1. Shannon Measure of Information (SIC)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Entropy"><span class="toc-number">1.2.</span> <span class="toc-text">2. Entropy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Information-Gain"><span class="toc-number">1.3.</span> <span class="toc-text">3. Information Gain</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Example"><span class="toc-number">2.</span> <span class="toc-text">Example</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index width mx-auto px2 my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Feature Selection using Mutual Information
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">BLOG</span>
      </span>
      
    <div class="postdate">
        <time datetime="2015-11-06T19:57:04.000Z" itemprop="datePublished">2015-11-06</time>
    </div>


      
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/blog/tags/entropy/">entropy</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. Here we will introduce the basic concept of this.</p>
<a id="more"></a>
<h1 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h1><h2 id="1-Shannon-Measure-of-Information-SIC"><a href="#1-Shannon-Measure-of-Information-SIC" class="headerlink" title="1. Shannon Measure of Information (SIC)"></a>1. Shannon Measure of Information (SIC)</h2><p>The measure of information should contain the following intuitive properties:</p>
<ol>
<li>Information contained in the events should be defined according to some measure of uncertainty of the events.</li>
<li>Less certain events should contain more information than more certain events.</li>
<li>The information of unrelated events taken as a single event should equal the sum of the information of the unrelated events.</li>
<li>The probabilities of various possible events should be taken into account.</li>
</ol>
<p>A natural way to measure the uncertainty of an event X is the probability of X. Based on this, according to the properties 2 – 4 above, the information content in an event can be expressed as:<br>$$INFO {X} = -\log p_x = \log\frac{1}{p_x}$$<br>, which can also be extended to multiple independent events as:<br>$$SIC = \sum_xp_x\log_2\frac{1}{p_x} = -\sum_xp_x\log_2{p_x}$$</p>
<h2 id="2-Entropy"><a href="#2-Entropy" class="headerlink" title="2. Entropy"></a>2. Entropy</h2><p>Based on the derivation above, we can define entropy as:<br>$$H(X) = \sum_x{p(x)logp(x)}$$<br>According to its definition, entropy is a measure of the uncertainty of a random variable. Concretely, for a random variable, more uncertain (probability distribution is more flat) it is, larger the entropy value.</p>
<p>Similar with conditional probability, here we can also define the conditional entropy $H(Y|X)$ as the uncertainty of random variable Y when the random variable X is known.<br>$$H(Y|X) = \sum_x p(x)H(Y|X=x)$$</p>
<h2 id="3-Information-Gain"><a href="#3-Information-Gain" class="headerlink" title="3. Information Gain"></a>3. Information Gain</h2><p>In information theory, the information gain is actually called Mutual Information:</p>
<p>$$I(X, Y) = H(X) - H(X|Y)$$</p>
<p>Mutual information measures the information that X and Y share. In another words, it measures how much knowing one of these variables reduces uncertainty about the other. Concretely, if X and Y are independent, then the mutual information is zero; if X and Y are identical, then the $𝐼(𝑋,𝑌) =𝐻(𝑋) =𝐻(𝑌)$</p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><p>Let’s take a machine learning case for example: X is the dataset with labels and several features and we want to find out which feature contains the most capability to classify this dataset. So what should we do to get it? By using the information gain criteria, we can just simply select the one (feature Q) with the highest mutual information $I(X,Q)$</p>
<p>A brief explanation: $H(X)$ is the uncertainty of the dataset (and its label); $H(X|Q)$ is the uncertainty of the dataset when the information Q is known; and the mutual information is the deference between them, which also can be considered as the decrease of the uncertainty of classifiation with the knowledge of Q. This is also why it is called “information gain”. Therefore, it is obvious that the feature with the highest information gain contains the most capability to classify the dataset.</p>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="http://luofanghao.github.io">Home</a></li>
         
          <li><a href="/blog/">Blog</a></li>
         
          <li><a href="/blog/archives/">Writing</a></li>
         
          <li><a href="/blog/about/">AboutMe</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Theory"><span class="toc-number">1.</span> <span class="toc-text">Theory</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Shannon-Measure-of-Information-SIC"><span class="toc-number">1.1.</span> <span class="toc-text">1. Shannon Measure of Information (SIC)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Entropy"><span class="toc-number">1.2.</span> <span class="toc-text">2. Entropy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Information-Gain"><span class="toc-number">1.3.</span> <span class="toc-text">3. Information Gain</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Example"><span class="toc-number">2.</span> <span class="toc-text">Example</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://luofanghao.github.io/blog/2015/11/06/entropy/"><i class="fa fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&text=Feature Selection using Mutual Information"><i class="fa fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&is_video=false&description=Feature Selection using Mutual Information"><i class="fa fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Feature Selection using Mutual Information&body=Check out this article: http://luofanghao.github.io/blog/2015/11/06/entropy/"><i class="fa fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&title=Feature Selection using Mutual Information"><i class="fa fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://luofanghao.github.io/blog/2015/11/06/entropy/&name=Feature Selection using Mutual Information&description=&lt;p&gt;Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. Here we will introduce the basic concept of this.&lt;/p&gt;"><i class="fa fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
      <ul>
        <li id="toc"><a class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa fa-list fa-lg" aria-hidden="true"></i> TOC</a></li>
        <li id="share"><a class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa fa-share-alt fa-lg" aria-hidden="true"></i> Share</a></li>
        <li id="top" style="display:none"><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a></li>
        <li id="menu"><a class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa fa-bars fa-lg" aria-hidden="true"></i> Menu</a></li>
      </ul>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2017 Fanghao Luo
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="http://luofanghao.github.io">Home</a></li>
         
          <li><a href="/blog/">Blog</a></li>
         
          <li><a href="/blog/archives/">Writing</a></li>
         
          <li><a href="/blog/about/">AboutMe</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css">
<link rel="stylesheet" href="/blog/lib/meslo-LG/styles.css">
<link rel="stylesheet" href="/blog/lib/justified-gallery/justifiedGallery.min.css">

<!-- jquery -->
<script src="/blog/lib/jquery/jquery.min.js"></script>
<script src="/blog/lib/justified-gallery/jquery.justifiedGallery.min.js"></script>
<script src="/blog/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-86660611-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Disqus Comments -->


